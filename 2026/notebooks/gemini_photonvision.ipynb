{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c111d39b-1b27-41bc-b6e4-78e9c7f8c565",
   "metadata": {},
   "source": [
    "#### attempting to replicate photonvision's best practices for my own pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb9155a-5d3e-4bd6-a1d6-dd3647d45558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2024 Crescendo Layout\n",
      "Tag Model Shape: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Constants\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# We will use robotpy_apriltag to get the official field layout\n",
    "# This ensures our \"Object Points\" match the official game data exactly.\n",
    "from robotpy_apriltag import AprilTagField, AprilTagFieldLayout\n",
    "\n",
    "# Standard FRC Coordinate Systems\n",
    "# WPILib: X=Forward, Y=Left, Z=Up\n",
    "# OpenCV: Z=Forward, X=Right, Y=Down\n",
    "# We will define the transform matrices explicitly to avoid confusion.\n",
    "\n",
    "class PhotonConstants:\n",
    "    # Standard AprilTag 36h11 size for FRC (16.51 cm)\n",
    "    TAG_SIZE_METERS = 0.1651 \n",
    "    \n",
    "    # 2025 Reefscape Layout (fallback to Crescendo if not found)\n",
    "    try:\n",
    "        FIELD_LAYOUT = AprilTagFieldLayout.loadField(AprilTagField.k2025Reefscape)\n",
    "        print(\"Loaded 2025 Reefscape Layout\")\n",
    "    except Exception:\n",
    "        FIELD_LAYOUT = AprilTagFieldLayout.loadField(AprilTagField.k2024Crescendo)\n",
    "        print(\"Loaded 2024 Crescendo Layout\")\n",
    "\n",
    "    # Corner offsets in Tag-Local space (Standard Counter-Clockwise from Bottom-Left in WPILib?)\n",
    "    # WAIT: OpenCV expects corners Top-Left, Top-Right, Bottom-Right, Bottom-Left\n",
    "    # We must match the order the detector returns.\n",
    "    s = TAG_SIZE_METERS / 2.0\n",
    "    \n",
    "    # These are the 3D points of a tag centered at (0,0,0) facing +X in WPILib coords\n",
    "    # But OpenCV solvePnP usually assumes the object is in the XY plane (Z=0).\n",
    "    # PhotonVision defines corner locations in the *Field Coordinate System* directly for Multi-Tag.\n",
    "    \n",
    "    # For single tag solve, we define the tag in its own reference frame:\n",
    "    # (Z forward, X right, Y down - standard OpenCV local frame)\n",
    "    # This matches the standard solvePnP expectation.\n",
    "    TAG_MODEL_OPENCV = np.array([\n",
    "        [-s, -s, 0.0], # Top-Left\n",
    "        [ s, -s, 0.0], # Top-Right\n",
    "        [ s,  s, 0.0], # Bottom-Right\n",
    "        [-s,  s, 0.0]  # Bottom-Left\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# Verification\n",
    "print(f\"Tag Model Shape: {PhotonConstants.TAG_MODEL_OPENCV.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c49eb439-b23c-439d-8c31-85483757ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports fixed. State container ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 (Revised): Imports & State\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from wpimath.geometry import Pose3d, Rotation3d, Translation3d, Transform3d, Quaternion\n",
    "\n",
    "# Global State for History Tracking\n",
    "class VisionState:\n",
    "    last_robot_pose = None # Pose3d\n",
    "    last_timestamp = 0.0\n",
    "    \n",
    "print(\"Imports fixed. State container ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e83cfca-1319-45c0-8e2a-4809bf40611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Helpers Defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Coordinate Helpers\n",
    "from wpimath.geometry import Transform3d, Translation3d, Rotation3d, Pose3d, Quaternion\n",
    "\n",
    "class VisionMath:\n",
    "    @staticmethod\n",
    "    def get_cam_to_robot_pose(cam_pose_in_field: Pose3d, camera_to_robot: Transform3d) -> Pose3d:\n",
    "        \"\"\"\n",
    "        Converts the camera's position in the field to the robot's position.\n",
    "        cam_pose_in_field: Where the camera is (in WPILib field coords)\n",
    "        camera_to_robot: The transform FROM camera TO robot (inverse of robot_to_cam)\n",
    "        \"\"\"\n",
    "        return cam_pose_in_field.transformBy(camera_to_robot)\n",
    "\n",
    "    @staticmethod\n",
    "    def open_cv_to_wpilib(tvec, rvec):\n",
    "        \"\"\"\n",
    "        Converts OpenCV solvePnP output (Camera->Tag) to WPILib (Camera->Tag).\n",
    "        OpenCV: Z-forward, X-right, Y-down\n",
    "        WPILib: X-forward, Y-left, Z-up\n",
    "        \"\"\"\n",
    "        # Rotation Matrix (OpenCV)\n",
    "        R_cv, _ = cv2.Rodrigues(rvec)\n",
    "        \n",
    "        # Transformation Matrix to switch axes\n",
    "        # We want to map:\n",
    "        # CV_Z (Forward) -> WPI_X (Forward)\n",
    "        # CV_X (Right)   -> WPI_Y (Left) * -1 \n",
    "        # CV_Y (Down)    -> WPI_Z (Up) * -1\n",
    "        # This is a standard coordinate basis change.\n",
    "        # However, PhotonVision often does this by transforming the *result* pose.\n",
    "        \n",
    "        # Alternative (PhotonVision style):\n",
    "        # 1. Get the translation in Camera Frame\n",
    "        x = tvec[2][0]\n",
    "        y = -tvec[0][0]\n",
    "        z = -tvec[1][0]\n",
    "        \n",
    "        # 2. Convert Rotation\n",
    "        # This is the tricky part. We usually construct a Rotation3d from the matrix\n",
    "        # then rotate it to match the WPILib frame.\n",
    "        # For this notebook, we will write a rigorous test in Cell 5 to prove this transform.\n",
    "        return x, y, z\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_corners(corners):\n",
    "        \"\"\"\n",
    "        Ensure corners are always [TL, TR, BR, BL] based on the image axes.\n",
    "        This fixes the 'permutation' bugs seen in custom pipelines.\n",
    "        \"\"\"\n",
    "        # Center of mass\n",
    "        center = np.mean(corners, axis=0)\n",
    "        \n",
    "        # Sort by angle to ensure counter-clockwise (or clockwise) order\n",
    "        # Then rotate the array so the point with (min X + min Y) is first (Top-Left)\n",
    "        # For simplicity in this cell, we will trust the detector's winding \n",
    "        # but ensure the \"first\" corner is the Top-Left one visually.\n",
    "        \n",
    "        # Simple heuristic for standard upright tags:\n",
    "        # Top-Left has the smallest (x + y) sum.\n",
    "        sums = [pt[0] + pt[1] for pt in corners]\n",
    "        tl_idx = np.argmin(sums)\n",
    "        \n",
    "        return np.roll(corners, -tl_idx, axis=0)\n",
    "\n",
    "print(\"Math Helpers Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56abf51f-b82d-4c90-bf96-ce603dfd3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 3: Single Tag Solver (Strict IPPE Spec) Defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Single Tag Solver (Strict IPPE Compliance)\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def solve_single_tag(corners, K, D, tag_size):\n",
    "    \"\"\"\n",
    "    Solves for single tag pose using SOLVEPNP_IPPE_SQUARE.\n",
    "    \n",
    "    CRITICAL: IPPE_SQUARE requires specific corner order:\n",
    "    0: [-s/2,  s/2] -> Bottom-Left (in OpenCV Y-down)\n",
    "    1: [ s/2,  s/2] -> Bottom-Right\n",
    "    2: [ s/2, -s/2] -> Top-Right\n",
    "    3: [-s/2, -s/2] -> Top-Left\n",
    "    \n",
    "    Args:\n",
    "        corners: (4, 2) numpy array. EXPECTED ORDER: [TL, TR, BR, BL]\n",
    "    \"\"\"\n",
    "    s = tag_size\n",
    "    # Defined strictly by the IPPE_SQUARE docstring\n",
    "    object_points = np.array([\n",
    "        [-s/2,  s/2, 0], # Point 0: Bottom-Left\n",
    "        [ s/2,  s/2, 0], # Point 1: Bottom-Right\n",
    "        [ s/2, -s/2, 0], # Point 2: Top-Right\n",
    "        [-s/2, -s/2, 0]  # Point 3: Top-Left\n",
    "    ], dtype=np.float64).reshape(4, 1, 3)\n",
    "\n",
    "    # Reorder Image Points to match Object Points\n",
    "    # Input assumed: [TL(0), TR(1), BR(2), BL(3)]\n",
    "    # Target:        [BL(3), BR(2), TR(1), TL(0)]\n",
    "    # Note: If input is [TL, TR, BR, BL], then:\n",
    "    # BL is index 3\n",
    "    # BR is index 2\n",
    "    # TR is index 1\n",
    "    # TL is index 0\n",
    "    img_ordered = corners[[3, 2, 1, 0], :].astype(np.float64).reshape(4, 1, 2)\n",
    "\n",
    "    K = K.astype(np.float64)\n",
    "    D = D.astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "    try:\n",
    "        # Strict unpacking based on user docstring: retval, rvecs, tvecs, errs\n",
    "        ret, rvecs, tvecs, errs = cv2.solvePnPGeneric(\n",
    "            object_points, img_ordered, K, D, flags=cv2.SOLVEPNP_IPPE_SQUARE\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Solver Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    if len(rvecs) == 0: return None\n",
    "\n",
    "    # Calculate Ambiguity\n",
    "    # IPPE returns 2 solutions. We want the one with lowest error.\n",
    "    # Note: 'errs' from solvePnPGeneric corresponds to the reordered points.\n",
    "    \n",
    "    # We must trust our own reprojection check because 'errs' behavior varies by version\n",
    "    actual_errors = []\n",
    "    # Project back to verify (using the IPPE object point order)\n",
    "    obj_pts_flat = object_points.reshape(4, 3)\n",
    "    \n",
    "    for i in range(len(rvecs)):\n",
    "        proj, _ = cv2.projectPoints(obj_pts_flat, rvecs[i], tvecs[i], K, D)\n",
    "        err = cv2.norm(img_ordered.reshape(4,2), proj.reshape(4,2), cv2.NORM_L2)\n",
    "        actual_errors.append(err)\n",
    "\n",
    "    best_idx = np.argmin(actual_errors)\n",
    "    best_err = actual_errors[best_idx]\n",
    "    \n",
    "    # Ambiguity Score\n",
    "    if len(actual_errors) > 1:\n",
    "        alt_idx = 1 - best_idx\n",
    "        alt_err = actual_errors[alt_idx]\n",
    "        ambiguity = 1.0 if alt_err < 1e-6 else best_err / alt_err\n",
    "    else:\n",
    "        ambiguity = 0.0\n",
    "\n",
    "    return {\n",
    "        'rvec': rvecs[best_idx], \n",
    "        'tvec': tvecs[best_idx], \n",
    "        'error': best_err, \n",
    "        'ambiguity': ambiguity\n",
    "    }\n",
    "\n",
    "print(\"Cell 3: Single Tag Solver (Strict IPPE Spec) Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0e068cd-5104-4cdc-946f-33d5dbfa02fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Logic: Concept defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 (Advanced): History-Aware Solver\n",
    "def solve_single_tag_robust(corners, K, D, tag_size, prev_pose=None):\n",
    "    \"\"\"\n",
    "    Solves IPPE and uses history (prev_pose) to pick the correct solution \n",
    "    if ambiguity is high.\n",
    "    \n",
    "    prev_pose: Pose3d (The Robot's field pose from the last frame)\n",
    "    \"\"\"\n",
    "    s = tag_size\n",
    "    # IPPE Order: BL, BR, TR, TL\n",
    "    object_points = np.array([\n",
    "        [-s/2,  s/2, 0], [ s/2,  s/2, 0], [ s/2, -s/2, 0], [-s/2, -s/2, 0]\n",
    "    ], dtype=np.float64).reshape(4, 1, 3)\n",
    "\n",
    "    # Detector usually returns TL, TR, BR, BL (Counter-Clockwise from TL)\n",
    "    # We map Detector(0)->IPPE(3), Det(1)->IPPE(2), Det(2)->IPPE(1), Det(3)->IPPE(0)\n",
    "    # Wait, simple reversal? \n",
    "    # Detector: TL, TR, BR, BL\n",
    "    # IPPE:     BL, BR, TR, TL\n",
    "    # Yes, it is exactly reversed order.\n",
    "    img_ordered = corners[[3, 2, 1, 0], :].astype(np.float64).reshape(4, 1, 2)\n",
    "    \n",
    "    K = K.astype(np.float64); D = D.astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "    try:\n",
    "        ret, rvecs, tvecs, _ = cv2.solvePnPGeneric(\n",
    "            object_points, img_ordered, K, D, flags=cv2.SOLVEPNP_IPPE_SQUARE\n",
    "        )\n",
    "    except Exception: return None\n",
    "\n",
    "    # ... (Unpacking logic from before) ...\n",
    "    # (Simplified for brevity, assume valid rvecs found)\n",
    "    \n",
    "    solutions = []\n",
    "    for i in range(len(rvecs)):\n",
    "        # Calculate Error\n",
    "        proj, _ = cv2.projectPoints(object_points, rvecs[i], tvecs[i], K, D)\n",
    "        err = cv2.norm(img_ordered.reshape(4,2), proj.reshape(4,2), cv2.NORM_L2)\n",
    "        \n",
    "        # Calculate \"Distance to History\" if available\n",
    "        dist_score = float('inf')\n",
    "        if prev_pose is not None:\n",
    "            # We need to convert this rvec/tvec to a Robot Field Pose to compare.\n",
    "            # This requires the Field Layout and Camera Offset, which we don't have inside this function.\n",
    "            # So PhotonVision typically returns BOTH solutions and lets the upper manager decide.\n",
    "            pass\n",
    "        \n",
    "        solutions.append({'rvec': rvecs[i], 'tvec': tvecs[i], 'err': err})\n",
    "\n",
    "    # Sort by Reprojection Error first\n",
    "    solutions.sort(key=lambda x: x['err'])\n",
    "    \n",
    "    best = solutions[0]\n",
    "    alt  = solutions[1] if len(solutions) > 1 else best\n",
    "    \n",
    "    # AMBIGUITY CHECK\n",
    "    ratio = best['err'] / (alt['err'] + 1e-9)\n",
    "    \n",
    "    # If ratio is high (e.g. > 0.15), we are ambiguous.\n",
    "    # IF ambiguous AND we have history:\n",
    "    #   Pick the solution closest to history, even if its error is slightly worse.\n",
    "    \n",
    "    return solutions # Return ALL candidates so the manager can decide using history\n",
    "\n",
    "print(\"History Logic: Concept defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "818c81cb-e43c-4d07-809f-d3fbd14fb65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test 1: 45 Degree Angle ---\n",
      "Input Z: 2.0000 m | Solved Z: 2.0000 m\n",
      "Input Ry: 45.00 deg | Solved Ry: 45.00 deg\n",
      "Ambiguity: 0.0000\n",
      "PASSED: 45 Deg Test\n",
      "\n",
      "--- Test 2: Face-On (Noisy) ---\n",
      "Ambiguity: 0.1638\n",
      "PASSED: Ambiguity detected properly.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Verification (Strict)\n",
    "fx, fy, cx, cy = 500.0, 500.0, 320.0, 240.0\n",
    "K_sim = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float64)\n",
    "D_sim = np.zeros(5)\n",
    "\n",
    "# 1. Define Standard Object for generating \"Perfect\" corners\n",
    "# We generate them as TL, TR, BR, BL (Standard visual order)\n",
    "s = 0.1651 / 2.0\n",
    "gen_obj = np.array([\n",
    "    [-s, -s, 0], # TL\n",
    "    [ s, -s, 0], # TR\n",
    "    [ s,  s, 0], # BR\n",
    "    [-s,  s, 0]  # BL\n",
    "], dtype=np.float64)\n",
    "\n",
    "# 2. Known Pose (2m away, 45 deg turn)\n",
    "known_tvec = np.array([[0.0], [0.0], [2.0]], dtype=np.float64)\n",
    "known_rvec = np.array([[0.0], [np.radians(45)], [0.0]], dtype=np.float64)\n",
    "\n",
    "# 3. Project to get [TL, TR, BR, BL] pixels\n",
    "perfect_corners, _ = cv2.projectPoints(gen_obj, known_rvec, known_tvec, K_sim, D_sim)\n",
    "perfect_corners = perfect_corners.reshape(4, 2)\n",
    "\n",
    "# 4. Solve\n",
    "result = solve_single_tag(perfect_corners, K_sim, D_sim, 0.1651)\n",
    "\n",
    "print(f\"--- Test 1: 45 Degree Angle ---\")\n",
    "if result:\n",
    "    print(f\"Input Z: {known_tvec[2][0]:.4f} m | Solved Z: {result['tvec'][2][0]:.4f} m\")\n",
    "    print(f\"Input Ry: {np.degrees(known_rvec[1][0]):.2f} deg | Solved Ry: {np.degrees(result['rvec'][1][0]):.2f} deg\")\n",
    "    print(f\"Ambiguity: {result['ambiguity']:.4f}\")\n",
    "    \n",
    "    # Assertions\n",
    "    assert np.isclose(result['tvec'][2][0], 2.0, atol=0.05), f\"Depth Wrong: {result['tvec'][2][0]}\"\n",
    "    assert result['ambiguity'] < 0.2, \"Ambiguity too high for clear target\"\n",
    "    print(\"PASSED: 45 Deg Test\")\n",
    "else:\n",
    "    print(\"FAILED: Solver returned None\")\n",
    "\n",
    "# 5. Face-On Test (High Ambiguity)\n",
    "face_rvec = np.array([[0.0], [0.0], [0.0]], dtype=np.float64)\n",
    "face_corners, _ = cv2.projectPoints(gen_obj, face_rvec, known_tvec, K_sim, D_sim)\n",
    "face_corners = face_corners.reshape(4, 2)\n",
    "# Add noise to induce ambiguity\n",
    "noise = np.random.normal(0, 0.5, face_corners.shape) \n",
    "result_bad = solve_single_tag(face_corners + noise, K_sim, D_sim, 0.1651)\n",
    "\n",
    "print(f\"\\n--- Test 2: Face-On (Noisy) ---\")\n",
    "if result_bad:\n",
    "    print(f\"Ambiguity: {result_bad['ambiguity']:.4f}\")\n",
    "    if result_bad['ambiguity'] > 0.1:\n",
    "        print(\"PASSED: Ambiguity detected properly.\")\n",
    "    else:\n",
    "        print(\"WARNING: Ambiguity low, but noise was random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a310872-55e6-4c30-a1b2-090fee655abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 5: Multi-Tag Solver Defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Multi-Tag Solver (PhotonVision Strategy)\n",
    "import cv2\n",
    "import numpy as np\n",
    "from wpimath.geometry import Pose3d, Rotation3d, Translation3d, Transform3d\n",
    "\n",
    "def solve_multi_tag(detections, field_layout, K, D):\n",
    "    \"\"\"\n",
    "    Performs a single rigid-body solve using all visible tags.\n",
    "    \n",
    "    Args:\n",
    "        detections: List of dicts {'id': int, 'corners': (4,2) np array [TL, TR, BR, BL]}\n",
    "        field_layout: RobotPy AprilTagFieldLayout\n",
    "        K, D: Camera intrinsics\n",
    "        \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'rvec': np.array (Camera->Field), \n",
    "            'tvec': np.array (Camera->Field), \n",
    "            'rmse': float,\n",
    "            'robot_pose': Pose3d (Field->Robot)\n",
    "        }\n",
    "        or None if solve fails.\n",
    "    \"\"\"\n",
    "    obj_points = []\n",
    "    img_points = []\n",
    "    \n",
    "    # Define Corner Offsets in WPILib Tag Frame (X=Forward, Y=Left, Z=Up)\n",
    "    # Order matches standard detection: TL, TR, BR, BL\n",
    "    s = 0.1651 / 2.0\n",
    "    local_corners = [\n",
    "        Translation3d(0.0,  s,  s), # TL (Up-Left)\n",
    "        Translation3d(0.0, -s,  s), # TR (Up-Right)\n",
    "        Translation3d(0.0, -s, -s), # BR (Down-Right)\n",
    "        Translation3d(0.0,  s, -s), # BL (Down-Left)\n",
    "    ]\n",
    "\n",
    "    for det in detections:\n",
    "        tag_id = int(det['id'])\n",
    "        corners = det['corners'] # Expecting (4, 2)\n",
    "        \n",
    "        # Look up tag pose in field\n",
    "        tag_pose = field_layout.getTagPose(tag_id)\n",
    "        if tag_pose is None: continue\n",
    "\n",
    "        # Transform local tag corners to Global Field Coordinates\n",
    "        for i, pt_local in enumerate(local_corners):\n",
    "            # corner_field = TagPose * corner_local\n",
    "            global_pt = tag_pose.transformBy(Transform3d(pt_local, Rotation3d()))\n",
    "            \n",
    "            obj_points.append([global_pt.X(), global_pt.Y(), global_pt.Z()])\n",
    "            img_points.append(corners[i])\n",
    "\n",
    "    # Require at least 2 tags (8 points) for the \"Multi-Tag Advantage\"\n",
    "    # (Technically works with 1, but we prefer single-tag IPPE for that)\n",
    "    if len(obj_points) < 8: \n",
    "        return None\n",
    "        \n",
    "    obj_points = np.array(obj_points, dtype=np.float64)\n",
    "    img_points = np.array(img_points, dtype=np.float64)\n",
    "    K = K.astype(np.float64)\n",
    "    D = D.astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "    # Solve FIELD -> CAMERA transform\n",
    "    # SQPNP is more robust than ITERATIVE for multiple planar targets\n",
    "    try:\n",
    "        success, rvec, tvec = cv2.solvePnP(\n",
    "            obj_points, img_points, K, D, flags=cv2.SOLVEPNP_SQPNP\n",
    "        )\n",
    "    except cv2.error:\n",
    "        return None\n",
    "    \n",
    "    if not success: return None\n",
    "\n",
    "    # Calculate Reprojection Error (RMSE)\n",
    "    proj, _ = cv2.projectPoints(obj_points, rvec, tvec, K, D)\n",
    "    err = cv2.norm(img_points.reshape(-1,2), proj.reshape(-1,2), cv2.NORM_L2)\n",
    "    rmse = err / np.sqrt(len(obj_points))\n",
    "\n",
    "    # Convert Result to Robot Pose\n",
    "    # 1. Get Camera Pose in Field (C_field)\n",
    "    #    SolvePnP gives T_field_to_camera (X_cam = R * X_field + t)\n",
    "    #    We need X_field = R^T * (X_cam - t) -> Camera position is -R^T * t\n",
    "    R_cv, _ = cv2.Rodrigues(rvec)\n",
    "    t_field_to_cam = tvec.reshape(3, 1)\n",
    "    \n",
    "    # Camera Position in Field (XYZ)\n",
    "    C_field = -R_cv.T @ t_field_to_cam\n",
    "    \n",
    "    # Camera Rotation in Field\n",
    "    # R_cv maps Field(XYZ) -> Camera(Right, Down, Fwd)\n",
    "    # We want Field(XYZ) -> Camera(Fwd, Left, Up) (WPILib)\n",
    "    # R_wpi = P * R_cv  where P permutates axes\n",
    "    # Actually, simpler to construct the Basis Vectors:\n",
    "    # Cam Forward (X_wpi) is Z_cv (Column 2 of R_cv^T)\n",
    "    # Cam Left    (Y_wpi) is -X_cv (Column 0 of R_cv^T * -1)\n",
    "    # Cam Up      (Z_wpi) is -Y_cv (Column 1 of R_cv^T * -1)\n",
    "    \n",
    "    R_f2c_T = R_cv.T\n",
    "    c_fwd =  R_f2c_T[:, 2] # Camera Z axis expressed in Field Coords\n",
    "    c_up  = -R_f2c_T[:, 1] # Camera -Y axis expressed in Field Coords\n",
    "\n",
    "    # Construct LookAt-style Rotation using WPILib geometry? \n",
    "    # Or just return the raw translation for now and handle rotation in the helper.\n",
    "    # Let's do a direct construct for completeness.\n",
    "    \n",
    "    # For now, let's return the raw opencv components and the computed field translation\n",
    "    # to keep the \"Solver\" pure math.\n",
    "    \n",
    "    return {\n",
    "        'rvec': rvec,\n",
    "        'tvec': tvec,\n",
    "        'rmse': rmse,\n",
    "        'field_translation': C_field.flatten()\n",
    "    }\n",
    "    \n",
    "print(\"Cell 5: Multi-Tag Solver Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f9ccd94-76eb-47be-8765-cd10948da946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Rigorous Multi-Tag Verification ---\n",
      "Solved Camera Pos: [-0.0000, -0.0000, -0.0000]\n",
      "Expected Pos:      [0.0000, 0.0000, 0.0000]\n",
      "Total Error:       0.000000 m\n",
      "PASSED: Solver matches Ground Truth.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Rigorous Multi-Tag Verification\n",
    "import numpy as np\n",
    "import cv2\n",
    "from wpimath.geometry import Pose3d, Rotation3d, Translation3d, Transform3d\n",
    "\n",
    "# --- 1. Setup Ground Truth ---\n",
    "# Camera is at Origin (0,0,0) in Field, facing +X direction.\n",
    "# We must define the Transform from Field -> Camera for the projection.\n",
    "# Field +X = Camera +Z (Forward)\n",
    "# Field +Y = Camera -X (Left)\n",
    "# Field +Z = Camera -Y (Up)\n",
    "# This rotation matrix aligns the axes.\n",
    "R_field_to_cam_gt = np.array([\n",
    "    [ 0, -1,  0], # Field X -> Cam ? No.\n",
    "    [ 0,  0, -1],\n",
    "    [ 1,  0,  0]\n",
    "], dtype=np.float64)\n",
    "# Let's verify: \n",
    "# Field(1,0,0) -> R * [1,0,0] = [0,0,1] (Cam Z). Correct.\n",
    "# Field(0,1,0) -> R * [0,1,0] = [-1,0,0] (Cam -X). Correct.\n",
    "# Field(0,0,1) -> R * [0,0,1] = [0,-1,0] (Cam -Y). Correct.\n",
    "\n",
    "t_field_to_cam_gt = np.zeros((3, 1), dtype=np.float64) # Camera is at 0,0,0\n",
    "\n",
    "# Camera Intrinsics\n",
    "K_sim = np.array([[500, 0, 320], [0, 500, 240], [0, 0, 1]], dtype=np.float64)\n",
    "D_sim = np.zeros((5,1), dtype=np.float64)\n",
    "\n",
    "# --- 2. Setup Mock Layout (Two Tags) ---\n",
    "# Tag 1: 2m in front (X=2), facing Camera (-X).\n",
    "t1_pose = Pose3d(Translation3d(2.0, 0.0, 0.0), Rotation3d(0, 0, np.pi))\n",
    "# Tag 2: 2m in front, 1m Right (X=2, Y=-1), facing Camera.\n",
    "t2_pose = Pose3d(Translation3d(2.0, -1.0, 0.0), Rotation3d(0, 0, np.pi))\n",
    "\n",
    "class MockLayoutRigorous:\n",
    "    def getTagPose(self, id):\n",
    "        if id == 1: return t1_pose\n",
    "        if id == 2: return t2_pose\n",
    "        return None\n",
    "layout_gt = MockLayoutRigorous()\n",
    "\n",
    "# --- 3. Generate Perfect Pixel Inputs ---\n",
    "# We use the SAME logic as the solver to get Field Points, \n",
    "# then project them using our known Ground Truth Camera.\n",
    "\n",
    "detections_gt = []\n",
    "s = 0.1651 / 2.0\n",
    "local_corners = [\n",
    "    Translation3d(0.0,  s,  s), # TL\n",
    "    Translation3d(0.0, -s,  s), # TR\n",
    "    Translation3d(0.0, -s, -s), # BR\n",
    "    Translation3d(0.0,  s, -s), # BL\n",
    "]\n",
    "\n",
    "for tid in [1, 2]:\n",
    "    pose = layout_gt.getTagPose(tid)\n",
    "    obj_pts_field = []\n",
    "    \n",
    "    for pt_local in local_corners:\n",
    "        # Convert Tag-Local -> Field\n",
    "        global_pt = pose.transformBy(Transform3d(pt_local, Rotation3d()))\n",
    "        obj_pts_field.append([global_pt.X(), global_pt.Y(), global_pt.Z()])\n",
    "    \n",
    "    obj_pts_field = np.array(obj_pts_field, dtype=np.float64).reshape(-1, 3)\n",
    "    \n",
    "    # Project Field -> Camera (using Ground Truth)\n",
    "    # Note: projectPoints expects T_cam_to_obj? No, T_obj_to_cam.\n",
    "    # We have T_field_to_cam.\n",
    "    rvec_gt, _ = cv2.Rodrigues(R_field_to_cam_gt)\n",
    "    img_pts, _ = cv2.projectPoints(obj_pts_field, rvec_gt, t_field_to_cam_gt, K_sim, D_sim)\n",
    "    \n",
    "    detections_gt.append({'id': tid, 'corners': img_pts.reshape(4, 2)})\n",
    "\n",
    "# --- 4. Run The Solver ---\n",
    "result = solve_multi_tag(detections_gt, layout_gt, K_sim, D_sim)\n",
    "\n",
    "print(\"--- Rigorous Multi-Tag Verification ---\")\n",
    "if result:\n",
    "    pos = result['field_translation']\n",
    "    print(f\"Solved Camera Pos: [{pos[0]:.4f}, {pos[1]:.4f}, {pos[2]:.4f}]\")\n",
    "    print(f\"Expected Pos:      [0.0000, 0.0000, 0.0000]\")\n",
    "    \n",
    "    err = np.linalg.norm(pos)\n",
    "    print(f\"Total Error:       {err:.6f} m\")\n",
    "    \n",
    "    assert err < 0.01, f\"Verification Failed. Error: {err}\"\n",
    "    print(\"PASSED: Solver matches Ground Truth.\")\n",
    "else:\n",
    "    print(\"FAILED: Solver returned None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd766341-7a58-48ec-ab73-a8fa11aac1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (robo2025)",
   "language": "python",
   "name": "robo2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
